{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156506e8-1d1b-4b68-a342-e0bd413d518f",
   "metadata": {},
   "source": [
    "# Final Project- Formal Methods in Robotics and AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0131543-dcf1-4dc7-a87e-e92afc0cd294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running value iteration (on-the-fly transitions, no big matrices)...\n",
      "Iter 0, delta = 8.000e-01\n",
      "Iter 1, delta = 7.200e-01\n",
      "Iter 2, delta = 6.400e-01\n",
      "Iter 3, delta = 2.833e-01\n",
      "Iter 4, delta = 1.632e-01\n",
      "Iter 5, delta = 1.249e-01\n",
      "Iter 6, delta = 1.050e-01\n",
      "Iter 7, delta = 4.589e-02\n",
      "Iter 8, delta = 3.180e-02\n",
      "Iter 9, delta = 2.360e-02\n",
      "Iter 10, delta = 1.952e-02\n",
      "Iter 11, delta = 1.680e-02\n",
      "Iter 12, delta = 1.401e-02\n",
      "Iter 13, delta = 1.172e-02\n",
      "Iter 14, delta = 9.563e-03\n",
      "Iter 15, delta = 7.836e-03\n",
      "Iter 16, delta = 6.322e-03\n",
      "Iter 17, delta = 5.113e-03\n",
      "Iter 18, delta = 4.095e-03\n",
      "Iter 19, delta = 3.299e-03\n",
      "Iter 20, delta = 2.642e-03\n",
      "Iter 21, delta = 2.124e-03\n",
      "Iter 22, delta = 1.706e-03\n",
      "Iter 23, delta = 1.378e-03\n",
      "Iter 24, delta = 1.112e-03\n",
      "Iter 25, delta = 9.105e-04\n",
      "Iter 26, delta = 7.476e-04\n",
      "Iter 27, delta = 6.148e-04\n",
      "Iter 28, delta = 5.056e-04\n",
      "Iter 29, delta = 4.161e-04\n",
      "Iter 30, delta = 3.425e-04\n",
      "Iter 31, delta = 2.821e-04\n",
      "Iter 32, delta = 2.323e-04\n",
      "Iter 33, delta = 1.914e-04\n",
      "Iter 34, delta = 1.577e-04\n",
      "Iter 35, delta = 1.299e-04\n",
      "Iter 36, delta = 1.071e-04\n",
      "Iter 37, delta = 8.826e-05\n",
      "Iter 38, delta = 7.732e-05\n",
      "Iter 39, delta = 6.571e-05\n",
      "Iter 40, delta = 5.557e-05\n",
      "Iter 41, delta = 4.687e-05\n",
      "Iter 42, delta = 3.945e-05\n",
      "Iter 43, delta = 3.315e-05\n",
      "Iter 44, delta = 2.783e-05\n",
      "Iter 45, delta = 2.334e-05\n",
      "Iter 46, delta = 1.956e-05\n",
      "Iter 47, delta = 1.639e-05\n",
      "Iter 48, delta = 1.372e-05\n",
      "Iter 49, delta = 1.148e-05\n",
      "Iter 50, delta = 9.607e-06\n",
      "Iter 51, delta = 8.036e-06\n",
      "Iter 52, delta = 6.720e-06\n",
      "Iter 53, delta = 5.618e-06\n",
      "Iter 54, delta = 4.696e-06\n",
      "Iter 55, delta = 3.924e-06\n",
      "Iter 56, delta = 3.279e-06\n",
      "Iter 57, delta = 2.739e-06\n",
      "Iter 58, delta = 2.288e-06\n",
      "Iter 59, delta = 1.910e-06\n",
      "Iter 60, delta = 1.595e-06\n",
      "Iter 61, delta = 1.332e-06\n",
      "Iter 62, delta = 1.112e-06\n",
      "Iter 63, delta = 9.281e-07\n",
      "Iter 64, delta = 7.746e-07\n",
      "Iter 65, delta = 6.465e-07\n",
      "Iter 66, delta = 5.395e-07\n",
      "Iter 67, delta = 4.501e-07\n",
      "Iter 68, delta = 3.756e-07\n",
      "Iter 69, delta = 3.133e-07\n",
      "Iter 70, delta = 2.614e-07\n",
      "Iter 71, delta = 2.180e-07\n",
      "Iter 72, delta = 1.819e-07\n",
      "Iter 73, delta = 1.517e-07\n",
      "Iter 74, delta = 1.265e-07\n",
      "Iter 75, delta = 1.055e-07\n",
      "Iter 76, delta = 8.801e-08\n",
      "Iter 77, delta = 7.339e-08\n",
      "Iter 78, delta = 6.120e-08\n",
      "Iter 79, delta = 5.104e-08\n",
      "Iter 80, delta = 4.256e-08\n",
      "Iter 81, delta = 3.549e-08\n",
      "Iter 82, delta = 2.959e-08\n",
      "Iter 83, delta = 2.467e-08\n",
      "Iter 84, delta = 2.057e-08\n",
      "Iter 85, delta = 1.715e-08\n",
      "Iter 86, delta = 1.430e-08\n",
      "Iter 87, delta = 1.192e-08\n",
      "Iter 88, delta = 9.938e-09\n",
      "Converged in 88 iterations.\n",
      "Value function length: 625\n",
      "Policy length: 625\n",
      "Value at initial state (Jerry=20, Tom=2): 0.9920\n",
      "\n",
      "Jerry's aggregated success probabilities over starting positions (5x5 grid):\n",
      "[[0.    0.929 0.947 0.949 0.947]\n",
      " [0.93  0.95  0.949 0.951 0.953]\n",
      " [0.949 0.96  0.934 0.95  0.96 ]\n",
      " [0.94  0.927 0.    0.929 0.947]\n",
      " [0.932 0.929 0.892 0.932 0.937]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. Grid and indexing\n",
    "# -------------------------------------------------------------\n",
    "GRID_SIZE = 5\n",
    "NUM_CELLS = GRID_SIZE * GRID_SIZE\n",
    "\n",
    "def rc_to_pos(r, c):\n",
    "    return r * GRID_SIZE + c\n",
    "\n",
    "def pos_to_rc(pos):\n",
    "    return divmod(pos, GRID_SIZE)\n",
    "\n",
    "# 2. Environment layout: traps and cheese \n",
    "TRAP_CELLS = {\n",
    "    rc_to_pos(0, 0),  # top-left in code coords (R_fig=4, C=0)\n",
    "    rc_to_pos(3, 2),  # lower trap (R_fig=1, C=2)\n",
    "}\n",
    "\n",
    "CHEESE_CELLS = {\n",
    "    rc_to_pos(2, 1),  # left cheese  (R_fig=2, C=1)\n",
    "    rc_to_pos(2, 4),  # right cheese (R_fig=2, C=3)\n",
    "}\n",
    "\n",
    "\n",
    "is_trap = np.zeros(NUM_CELLS, dtype=bool)\n",
    "is_cheese = np.zeros(NUM_CELLS, dtype=bool)\n",
    "for p in TRAP_CELLS:\n",
    "    is_trap[p] = True\n",
    "for p in CHEESE_CELLS:\n",
    "    is_cheese[p] = True\n",
    "\n",
    "# 3. Actions and single-agent motion\n",
    "NORTH, SOUTH, EAST, WEST = 0, 1, 2, 3\n",
    "ACTIONS = [NORTH, SOUTH, EAST, WEST]\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "ACTION_DELTAS = {\n",
    "    NORTH: (-1, 0),\n",
    "    SOUTH: ( 1, 0),\n",
    "    EAST:  ( 0, 1),\n",
    "    WEST:  ( 0,-1),\n",
    "}\n",
    "\n",
    "LEFT_OF = {\n",
    "    NORTH: WEST,\n",
    "    SOUTH: EAST,\n",
    "    EAST:  NORTH,\n",
    "    WEST:  SOUTH,\n",
    "}\n",
    "RIGHT_OF = {\n",
    "    NORTH: EAST,\n",
    "    SOUTH: WEST,\n",
    "    EAST:  SOUTH,\n",
    "    WEST:  NORTH,\n",
    "}\n",
    "\n",
    "def move_cell(pos, action):\n",
    "    \"\"\"Deterministic move; if off-grid, stay.\"\"\"\n",
    "    r, c = pos_to_rc(pos)\n",
    "    dr, dc = ACTION_DELTAS[action]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if 0 <= nr < GRID_SIZE and 0 <= nc < GRID_SIZE:\n",
    "        return rc_to_pos(nr, nc)\n",
    "    else:\n",
    "        return pos\n",
    "\n",
    "def jerry_transition_probs(jerry_pos, action):\n",
    "    \"\"\"\n",
    "    Return dict next_jerry_pos -> prob for Jerry:\n",
    "    0.8 intended direction, 0.1 left, 0.1 right.\n",
    "    \"\"\"\n",
    "    intended = action\n",
    "    left = LEFT_OF[action]\n",
    "    right = RIGHT_OF[action]\n",
    "    moves = [intended, left, right]\n",
    "    probs = [0.8, 0.1, 0.1]\n",
    "\n",
    "    out = {}\n",
    "    for a, p in zip(moves, probs):\n",
    "        nxt = move_cell(jerry_pos, a)\n",
    "        out[nxt] = out.get(nxt, 0.0) + p\n",
    "    return out\n",
    "\n",
    "def tom_transition_probs(tom_pos):\n",
    "    \"\"\"\n",
    "    Tom: uniform random N,S,E,W.\n",
    "    Return dict next_tom_pos -> prob.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for a in ACTIONS:\n",
    "        nxt = move_cell(tom_pos, a)\n",
    "        out[nxt] = out.get(nxt, 0.0) + 1.0 / NUM_ACTIONS\n",
    "    return out\n",
    "\n",
    "# 4. Joint state space (Jerry_pos, Tom_pos)\n",
    "NUM_STATES = NUM_CELLS * NUM_CELLS\n",
    "\n",
    "def encode_state(jerry_pos, tom_pos):\n",
    "    return jerry_pos * NUM_CELLS + tom_pos\n",
    "\n",
    "def decode_state(s):\n",
    "    j = s // NUM_CELLS\n",
    "    t = s % NUM_CELLS\n",
    "    return j, t\n",
    "\n",
    "def is_bad_state(j, t):\n",
    "    return is_trap[j] or (j == t)\n",
    "\n",
    "def is_goal_state(j, t):\n",
    "    return is_cheese[j]\n",
    "\n",
    "# 5. Transition for a given state & action (no big P tensor!)\n",
    "def joint_transition(s, a):\n",
    "    \"\"\"\n",
    "    For joint state index s and Jerry action a,\n",
    "    return a dict: next_state_index -> prob.\n",
    "    This computes transitions on the fly, using Jerry+Tom models.\n",
    "    \"\"\"\n",
    "    j, t = decode_state(s)\n",
    "\n",
    "    # If already terminal (goal or bad), stay put\n",
    "    if is_goal_state(j, t) or is_bad_state(j, t):\n",
    "        return {s: 1.0}\n",
    "\n",
    "    out = {}\n",
    "    j_probs = jerry_transition_probs(j, a)\n",
    "    t_probs = tom_transition_probs(t)\n",
    "\n",
    "    for j_next, pj in j_probs.items():\n",
    "        for t_next, pt in t_probs.items():\n",
    "            s_next = encode_state(j_next, t_next)\n",
    "            out[s_next] = out.get(s_next, 0.0) + pj * pt\n",
    "    return out\n",
    "\n",
    "#6. Value iteration for reachability (on-the-fly transitions)\n",
    "\n",
    "def value_iteration_reachability(tol=1e-8, max_iter=10_000):\n",
    "    \"\"\"\n",
    "    Solve V(s) = max_a E[V(s')] for each state,\n",
    "    with V(goal)=1, V(bad)=0.\n",
    "    Returns:\n",
    "        V: np.array shape (NUM_STATES,)\n",
    "        policy: np.array shape (NUM_STATES,) with best action.\n",
    "    \"\"\"\n",
    "    V = np.zeros(NUM_STATES, dtype=float)\n",
    "\n",
    "    # Precompute terminal masks\n",
    "    goal_mask = np.zeros(NUM_STATES, dtype=bool)\n",
    "    bad_mask = np.zeros(NUM_STATES, dtype=bool)\n",
    "    for s in range(NUM_STATES):\n",
    "        j, t = decode_state(s)\n",
    "        if is_goal_state(j, t):\n",
    "            goal_mask[s] = True\n",
    "        if is_bad_state(j, t):\n",
    "            bad_mask[s] = True\n",
    "\n",
    "    # Initialize boundary conditions\n",
    "    V[goal_mask] = 1.0\n",
    "    V[bad_mask] = 0.0\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        delta = 0.0\n",
    "        V_old = V.copy()\n",
    "\n",
    "        for s in range(NUM_STATES):\n",
    "            if goal_mask[s] or bad_mask[s]:\n",
    "                # Terminal; keep fixed\n",
    "                continue\n",
    "\n",
    "            best_q = -1.0\n",
    "            for a in ACTIONS:\n",
    "                q = 0.0\n",
    "                for s_next, p in joint_transition(s, a).items():\n",
    "                    q += p * V_old[s_next]\n",
    "                if q > best_q:\n",
    "                    best_q = q\n",
    "            V[s] = best_q\n",
    "            delta = max(delta, abs(V[s] - V_old[s]))\n",
    "\n",
    "        print(f\"Iter {it}, delta = {delta:.3e}\")\n",
    "        if delta < tol:\n",
    "            print(f\"Converged in {it} iterations.\")\n",
    "            break\n",
    "\n",
    "    # Extract greedy policy\n",
    "    policy = np.zeros(NUM_STATES, dtype=int)\n",
    "    for s in range(NUM_STATES):\n",
    "        if goal_mask[s] or bad_mask[s]:\n",
    "            policy[s] = -1  # no action needed\n",
    "            continue\n",
    "        best_a = 0\n",
    "        best_q = -1.0\n",
    "        for a in ACTIONS:\n",
    "            q = 0.0\n",
    "            for s_next, p in joint_transition(s, a).items():\n",
    "                q += p * V[s_next]\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_a = a\n",
    "        policy[s] = best_a\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "# 7. Aggregate over Tom's initial position\n",
    "def jerry_value_marginal_over_tom(V, tom_initial_dist=None):\n",
    "    if tom_initial_dist is None:\n",
    "        tom_initial_dist = np.ones(NUM_CELLS) / NUM_CELLS\n",
    "    tom_initial_dist = np.asarray(tom_initial_dist)\n",
    "    assert tom_initial_dist.shape == (NUM_CELLS,)\n",
    "    assert np.allclose(tom_initial_dist.sum(), 1.0)\n",
    "\n",
    "    jerry_vals = np.zeros(NUM_CELLS)\n",
    "    for j in range(NUM_CELLS):\n",
    "        val = 0.0\n",
    "        for t in range(NUM_CELLS):\n",
    "            s = encode_state(j, t)\n",
    "            val += tom_initial_dist[t] * V[s]\n",
    "        jerry_vals[j] = val\n",
    "    return jerry_vals.reshape(GRID_SIZE, GRID_SIZE)\n",
    "\n",
    "# 8. Run experiment\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running value iteration (on-the-fly transitions, no big matrices)...\")\n",
    "    V, policy = value_iteration_reachability()\n",
    "\n",
    "    print(\"Value function length:\", len(V))\n",
    "    print(\"Policy length:\", len(policy))\n",
    "\n",
    "    # Example: specific initial state\n",
    "    jerry_start = rc_to_pos(4, 0)\n",
    "    tom_start = rc_to_pos(0, 2)\n",
    "    s0 = encode_state(jerry_start, tom_start)\n",
    "    print(f\"Value at initial state (Jerry={jerry_start}, Tom={tom_start}): {V[s0]:.4f}\")\n",
    "\n",
    "    # Aggregate over random Tom initial position\n",
    "    grid_vals = jerry_value_marginal_over_tom(V)\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    print(\"\\nJerry's aggregated success probabilities over starting positions (5x5 grid):\")\n",
    "    print(grid_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ea7d875-1250-40f1-bdd8-f9174bfb1eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jerry's aggregated success probabilities over starting positions (5x5 grid):\n",
      "[[0.    0.929 0.947 0.949 0.947]\n",
      " [0.93  0.95  0.949 0.951 0.953]\n",
      " [0.949 0.96  0.934 0.95  0.96 ]\n",
      " [0.94  0.927 0.    0.929 0.947]\n",
      " [0.932 0.929 0.892 0.932 0.937]]\n",
      "Saved heatmap data to jerry_success_grid_randomTom.csv\n"
     ]
    }
   ],
   "source": [
    "# After value_iteration_reachability finishes:\n",
    "grid_vals = jerry_value_marginal_over_tom(V)\n",
    "\n",
    "# Print nicely to console (you already saw this)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"\\nJerry's aggregated success probabilities over starting positions (5x5 grid):\")\n",
    "print(grid_vals)\n",
    "\n",
    "# Save as CSV so you can plot it in Excel / Google Sheets / MATLAB\n",
    "np.savetxt(\"jerry_success_grid_randomTom.csv\", grid_vals, delimiter=\",\", fmt=\"%.4f\")\n",
    "print(\"Saved heatmap data to jerry_success_grid_randomTom.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62e71c58-2472-488f-8411-d5bed4a72c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jerry_policy_from_joint_ascii(V, tom_initial_dist=None):\n",
    "    \"\"\"\n",
    "    Compute an aggregated policy over Jerry positions only,\n",
    "    by averaging over Tom's initial position distribution.\n",
    "\n",
    "    Returns:\n",
    "        arrows_grid_code: 5x5 array of unicode arrows using code coordinates\n",
    "                          (row 0 = top).\n",
    "        arrows_grid_figure: 5x5 array flipped vertically so that\n",
    "                            row 0 is at the bottom, matching the slide\n",
    "                            convention (R:0 at bottom).\n",
    "    \"\"\"\n",
    "    if tom_initial_dist is None:\n",
    "        tom_initial_dist = np.ones(NUM_CELLS) / NUM_CELLS\n",
    "    tom_initial_dist = np.asarray(tom_initial_dist)\n",
    "    assert tom_initial_dist.shape == (NUM_CELLS,)\n",
    "    assert np.allclose(tom_initial_dist.sum(), 1.0)\n",
    "\n",
    "    # Map action index -> arrow character\n",
    "    arrow_for_action = {\n",
    "        NORTH: \"↑\",\n",
    "        SOUTH: \"↓\",\n",
    "        EAST:  \"→\",\n",
    "        WEST:  \"←\",\n",
    "    }\n",
    "\n",
    "    arrows_code = np.empty((GRID_SIZE, GRID_SIZE), dtype=object)\n",
    "\n",
    "    # For each Jerry cell j, pick the action maximizing expected next-state value\n",
    "    for j in range(NUM_CELLS):\n",
    "        best_a = NORTH\n",
    "        best_q = -1.0\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            q = 0.0\n",
    "            # Average over Tom's initial position\n",
    "            for t in range(NUM_CELLS):\n",
    "                s = encode_state(j, t)\n",
    "                trans = joint_transition(s, a)\n",
    "                for s_next, p in trans.items():\n",
    "                    q += tom_initial_dist[t] * p * V[s_next]\n",
    "\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_a = a\n",
    "\n",
    "        r, c = pos_to_rc(j)           # code coords: row 0 = TOP\n",
    "        arrows_code[r, c] = arrow_for_action[best_a]\n",
    "\n",
    "    # Flip vertically so row 0 is at the BOTTOM (matches slide R:0)\n",
    "    arrows_figure = np.flipud(arrows_code)\n",
    "\n",
    "    return arrows_code, arrows_figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b24da46e-5605-4d0d-9e22-b049e81178b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jerry's aggregated optimal directions (5x5 grid):\n",
      "↑ → → ↓ →\n",
      "↓ ↓ → → ↓\n",
      "→ ↑ ↑ → ↑\n",
      "↑ ← ↑ → ↑\n",
      "← ← ↓ → →\n"
     ]
    }
   ],
   "source": [
    "arrows_code, arrows_figure = jerry_policy_from_joint_ascii(V)\n",
    "\n",
    "print(\"\\nJerry's aggregated optimal directions (5x5 grid):\")\n",
    "for r in range(GRID_SIZE):\n",
    "    print(\" \".join(arrows_code[r]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (formal-methods)",
   "language": "python",
   "name": "formal-methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
