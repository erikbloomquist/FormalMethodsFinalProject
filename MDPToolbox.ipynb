{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eff12e2-5842-4435-90ff-6c1dd77692f9",
   "metadata": {},
   "source": [
    "# Final Project- Formal Methods in Robotics and AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da7c21-1fc0-4d9b-8317-7d12de50c358",
   "metadata": {},
   "source": [
    "### Erik Bloomquist, Adam Benali, Rodrigo Bazan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f228da-2f23-4d34-b796-a80dec01b372",
   "metadata": {},
   "source": [
    "Markov Decision Process constrained using LTL. Agent (Jerry) must never reach any hazard states (traps or Tom) while maximizing the probability of reaching a goal state (cheese). The run ends when either a hazard is reached or a cheese is reached. An optimal policy is computed using value iteration and displayed using arrows, representing the chosen action in each state. Success probabilities from each initial state are displayed. Tom's initial state is random uses a uniformely random movement policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6acda6-85d5-4f09-9c3c-327a38bf0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tom & Jerry temporal-logic-style gridworld using PyMDPtoolbox.\n",
    "\n",
    "- Grid: 5x5 (25 cells)\n",
    "- State: (Jerry_pos, Tom_pos) => 25*25 = 625 states\n",
    "- Actions: N, S, E, W for Jerry\n",
    "- Jerry's motion: 0.8 intended dir, 0.1 left, 0.1 right\n",
    "- Tom's motion: random over N,S,E,W (uniform)\n",
    "- Goal: Jerry reaches a cheese cell (absorbing success)\n",
    "- Bad: Jerry hits a trap or same cell as Tom (absorbing failure)\n",
    "\n",
    "We then solve a discounted MDP with PyMDPtoolboxâ€™s ValueIteration.\n",
    "With only a terminal reward of 1 at goal and discount ~1, the value\n",
    "function approximates the probability of safely reaching cheese.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Try to import PyMDPtoolbox (module name is usually mdptoolbox)\n",
    "try:\n",
    "    from mdptoolbox.mdp import ValueIteration\n",
    "except ImportError:\n",
    "    from pymdptoolbox.mdp import ValueIteration  # fallback if installed under old name\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Grid and indexing utilities\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "GRID_SIZE = 5\n",
    "NUM_CELLS = GRID_SIZE * GRID_SIZE\n",
    "\n",
    "def rc_to_pos(r, c):\n",
    "    \"\"\"Map (row, col) to single index 0..24.\"\"\"\n",
    "    return r * GRID_SIZE + c\n",
    "\n",
    "def pos_to_rc(pos):\n",
    "    \"\"\"Map 0..24 to (row, col).\"\"\"\n",
    "    return divmod(pos, GRID_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69aecd47-5f85-4caf-bd7c-1e27c2177f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 2. Environment layout: traps & cheese\n",
    "# ----------------------------------------------------------------------\n",
    "# You can change these to match your figure exactly.\n",
    "\n",
    "# Example: traps at (row,col) = (0,0) and (3,1)\n",
    "TRAP_CELLS = {\n",
    "    rc_to_pos(0, 0),\n",
    "    rc_to_pos(3, 1),\n",
    "}\n",
    "\n",
    "# Example: cheese at (row,col) = (2,1) and (2,3)\n",
    "CHEESE_CELLS = {\n",
    "    rc_to_pos(2, 1),\n",
    "    rc_to_pos(2, 3),\n",
    "}\n",
    "\n",
    "is_trap = np.zeros(NUM_CELLS, dtype=bool)\n",
    "is_cheese = np.zeros(NUM_CELLS, dtype=bool)\n",
    "for p in TRAP_CELLS:\n",
    "    is_trap[p] = True\n",
    "for p in CHEESE_CELLS:\n",
    "    is_cheese[p] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee92dbe-94f5-431e-af49-3ff8994ed7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 3. Actions and motion dynamics\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "NORTH, SOUTH, EAST, WEST = 0, 1, 2, 3\n",
    "ACTIONS = [NORTH, SOUTH, EAST, WEST]\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "ACTION_DELTAS = {\n",
    "    NORTH: (-1, 0),\n",
    "    SOUTH: ( 1, 0),\n",
    "    EAST:  ( 0, 1),\n",
    "    WEST:  ( 0,-1),\n",
    "}\n",
    "\n",
    "LEFT_OF = {\n",
    "    NORTH: WEST,\n",
    "    SOUTH: EAST,\n",
    "    EAST:  NORTH,\n",
    "    WEST:  SOUTH,\n",
    "}\n",
    "RIGHT_OF = {\n",
    "    NORTH: EAST,\n",
    "    SOUTH: WEST,\n",
    "    EAST:  SOUTH,\n",
    "    WEST:  NORTH,\n",
    "}\n",
    "\n",
    "def move_cell(pos, action):\n",
    "    \"\"\"Deterministic move for a single agent on the grid; if off-grid, stay.\"\"\"\n",
    "    r, c = pos_to_rc(pos)\n",
    "    dr, dc = ACTION_DELTAS[action]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if 0 <= nr < GRID_SIZE and 0 <= nc < GRID_SIZE:\n",
    "        return rc_to_pos(nr, nc)\n",
    "    else:\n",
    "        return pos  # bounce into wall\n",
    "\n",
    "\n",
    "def jerry_transition_probs(jerry_pos, action):\n",
    "    \"\"\"\n",
    "    Jerry's stochastic motion under chosen action.\n",
    "    Returns dict next_pos -> probability.\n",
    "    0.8 intended dir, 0.1 left, 0.1 right.\n",
    "    \"\"\"\n",
    "    intended = action\n",
    "    left = LEFT_OF[action]\n",
    "    right = RIGHT_OF[action]\n",
    "    moves = [intended, left, right]\n",
    "    probs = [0.8, 0.1, 0.1]\n",
    "\n",
    "    outcomes = {}\n",
    "    for a, p in zip(moves, probs):\n",
    "        nxt = move_cell(jerry_pos, a)\n",
    "        outcomes[nxt] = outcomes.get(nxt, 0.0) + p\n",
    "    return outcomes\n",
    "\n",
    "\n",
    "def tom_transition_probs(tom_pos):\n",
    "    \"\"\"\n",
    "    Tom's random policy: uniform over N,S,E,W.\n",
    "    Returns dict next_pos -> probability.\n",
    "    \"\"\"\n",
    "    outcomes = {}\n",
    "    for a in ACTIONS:\n",
    "        nxt = move_cell(tom_pos, a)\n",
    "        outcomes[nxt] = outcomes.get(nxt, 0.0) + 1.0 / NUM_ACTIONS\n",
    "    return outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d352bd69-7da5-4aba-811d-abd0bdb52f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 4. Joint state space (Jerry_pos, Tom_pos)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "NUM_STATES = NUM_CELLS * NUM_CELLS\n",
    "\n",
    "def encode_state(jerry_pos, tom_pos):\n",
    "    \"\"\"(jerry, tom) -> single state index 0..624.\"\"\"\n",
    "    return jerry_pos * NUM_CELLS + tom_pos\n",
    "\n",
    "def decode_state(s):\n",
    "    \"\"\"state index -> (jerry_pos, tom_pos).\"\"\"\n",
    "    jerry_pos = s // NUM_CELLS\n",
    "    tom_pos = s % NUM_CELLS\n",
    "    return jerry_pos, tom_pos\n",
    "\n",
    "\n",
    "def is_bad_state(jerry_pos, tom_pos):\n",
    "    \"\"\"Jerry is in trap OR collides with Tom.\"\"\"\n",
    "    return is_trap[jerry_pos] or (jerry_pos == tom_pos)\n",
    "\n",
    "\n",
    "def is_goal_state(jerry_pos, tom_pos):\n",
    "    \"\"\"Jerry is on any cheese cell; Tom can be anywhere.\"\"\"\n",
    "    return is_cheese[jerry_pos]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6d48c8-ee7b-42b4-b416-71370f2dcacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 5. Build transition matrices P for PyMDPtoolbox\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def build_transition_matrices():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        P: numpy array of shape (A, S, S)\n",
    "    where P[a, s, s_next] = Pr(s_next | s, a)\n",
    "    \"\"\"\n",
    "    P = np.zeros((NUM_ACTIONS, NUM_STATES, NUM_STATES), dtype=float)\n",
    "\n",
    "    # Base dynamics (before making goal/bad absorbing)\n",
    "    for jerry in range(NUM_CELLS):\n",
    "        for tom in range(NUM_CELLS):\n",
    "            s = encode_state(jerry, tom)\n",
    "\n",
    "            for a in ACTIONS:\n",
    "                j_trans = jerry_transition_probs(jerry, a)\n",
    "                t_trans = tom_transition_probs(tom)\n",
    "\n",
    "                for j_next, pj in j_trans.items():\n",
    "                    for t_next, pt in t_trans.items():\n",
    "                        s_next = encode_state(j_next, t_next)\n",
    "                        P[a, s, s_next] += pj * pt\n",
    "\n",
    "    # Make goal and bad states absorbing\n",
    "    for s in range(NUM_STATES):\n",
    "        jerry, tom = decode_state(s)\n",
    "        if is_goal_state(jerry, tom) or is_bad_state(jerry, tom):\n",
    "            P[:, s, :] = 0.0\n",
    "            for a in ACTIONS:\n",
    "                P[a, s, s] = 1.0\n",
    "\n",
    "    # Sanity check: each P[a] should be row-stochastic\n",
    "    # (Optional: you can comment this out for speed)\n",
    "    row_sums = P.sum(axis=2)\n",
    "    assert np.allclose(row_sums, 1.0, atol=1e-8), \"Transition matrices are not stochastic.\"\n",
    "\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede8b785-9a82-4517-8abc-6b03ad28f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 6. Reward vector R for reachability\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def build_reward_vector():\n",
    "    \"\"\"\n",
    "    R[s] = 1 if s is goal, else 0.\n",
    "    ValueIteration will then try to maximize expected discounted reward.\n",
    "    With only terminal rewards and discount close to 1, this approximates\n",
    "    the probability of eventually reaching cheese.\n",
    "    \"\"\"\n",
    "    R = np.zeros(NUM_STATES, dtype=float)\n",
    "    for s in range(NUM_STATES):\n",
    "        jerry, tom = decode_state(s)\n",
    "        if is_goal_state(jerry, tom):\n",
    "            R[s] = 1.0\n",
    "        else:\n",
    "            R[s] = 0.0\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57e3526-48ab-4a79-be4e-6d6bb32b2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 7. Solve the MDP with PyMDPtoolbox\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def solve_mdp(P, R, discount=0.99, epsilon=1e-5, max_iter=10_000):\n",
    "    \"\"\"\n",
    "    Run Value Iteration to approximate optimal safe-reachability policy.\n",
    "\n",
    "    Args:\n",
    "        P: transition matrices, shape (A, S, S)\n",
    "        R: reward vector, shape (S,)\n",
    "        discount: gamma in [0,1); close to 1 for long-horizon\n",
    "        epsilon: convergence threshold\n",
    "        max_iter: safety cap\n",
    "\n",
    "    Returns:\n",
    "        policy: array of shape (S,) with optimal action indices\n",
    "        V: array of shape (S,) with value function\n",
    "    \"\"\"\n",
    "    vi = ValueIteration(P, R, discount=discount, epsilon=epsilon, max_iter=max_iter)\n",
    "    vi.run()\n",
    "    return np.array(vi.policy), np.array(vi.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ae6037-60f2-442c-91eb-2348778b68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 8. Aggregating results for Jerry (optional but useful)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def jerry_value_marginal_over_tom(V, tom_initial_dist=None):\n",
    "    \"\"\"\n",
    "    Compute an aggregated \"success value\" for each Jerry cell,\n",
    "    averaging over Tom's initial position distribution.\n",
    "\n",
    "    Args:\n",
    "        V: value function over joint states, shape (S,)\n",
    "        tom_initial_dist: length-25 array giving Pr(Tom starts at cell k).\n",
    "                          If None, use uniform over all 25 cells.\n",
    "\n",
    "    Returns:\n",
    "        jerry_values: array shape (25,) where jerry_values[j]\n",
    "                      is the expected value when Jerry starts at cell j\n",
    "                      and Tom's initial location is random.\n",
    "    \"\"\"\n",
    "    if tom_initial_dist is None:\n",
    "        tom_initial_dist = np.ones(NUM_CELLS) / NUM_CELLS\n",
    "    tom_initial_dist = np.asarray(tom_initial_dist)\n",
    "    assert tom_initial_dist.shape == (NUM_CELLS,)\n",
    "    assert np.allclose(tom_initial_dist.sum(), 1.0)\n",
    "\n",
    "    jerry_values = np.zeros(NUM_CELLS)\n",
    "    for jerry in range(NUM_CELLS):\n",
    "        val = 0.0\n",
    "        for tom in range(NUM_CELLS):\n",
    "            s = encode_state(jerry, tom)\n",
    "            val += tom_initial_dist[tom] * V[s]\n",
    "        jerry_values[jerry] = val\n",
    "    return jerry_values\n",
    "\n",
    "\n",
    "def jerry_value_grid(jerry_values):\n",
    "    \"\"\"Reshape 25-element vector into 5x5 array for visualization.\"\"\"\n",
    "    return jerry_values.reshape(GRID_SIZE, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eae3b4-b99d-4f39-be09-e5f97140c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 9. Main experiment\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Build MDP\n",
    "    P = build_transition_matrices()\n",
    "    R = build_reward_vector()\n",
    "\n",
    "    # Solve it\n",
    "    policy, V = solve_mdp(P, R, discount=0.99)\n",
    "\n",
    "    print(\"Optimal policy shape:\", policy.shape)\n",
    "    print(\"Value function shape:\", V.shape)\n",
    "\n",
    "    # Example: value from a specific initial state\n",
    "    # Suppose Jerry starts at bottom-left (4,0), Tom at top-center (0,2)\n",
    "    jerry_start = rc_to_pos(4, 0)\n",
    "    tom_start = rc_to_pos(0, 2)\n",
    "    s0 = encode_state(jerry_start, tom_start)\n",
    "    print(f\"Value at initial state (Jerry={jerry_start}, Tom={tom_start}): {V[s0]:.4f}\")\n",
    "\n",
    "    # Aggregate over random Tom initial position\n",
    "    jerry_vals = jerry_value_marginal_over_tom(V)\n",
    "    grid_vals = jerry_value_grid(jerry_vals)\n",
    "\n",
    "    print(\"\\nJerry's aggregated values over starting positions (row-major 5x5):\")\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    print(grid_vals)\n",
    "\n",
    "    # If you want, you can also print the optimal action for Jerry at each joint state\n",
    "    # or just for Jerry's position assuming random Tom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27023e09-c1b3-4bfc-bdaf-66f2caea25e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building transition matrices...\n",
      "Done. Building rewards...\n",
      "Solving MDP with Value Iteration...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Try to import PyMDPtoolbox (module name can vary)\n",
    "try:\n",
    "    from mdptoolbox.mdp import ValueIteration\n",
    "except ImportError:\n",
    "    from pymdptoolbox.mdp import ValueIteration  # some installs use this name\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Grid / indexing utilities\n",
    "# ----------------------------------------------------------------------\n",
    "GRID_SIZE = 5\n",
    "NUM_CELLS = GRID_SIZE * GRID_SIZE\n",
    "\n",
    "def rc_to_pos(r, c):\n",
    "    return r * GRID_SIZE + c\n",
    "\n",
    "def pos_to_rc(pos):\n",
    "    return divmod(pos, GRID_SIZE)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Environment layout: traps & cheese (EDIT THESE TO MATCH YOUR FIGURE)\n",
    "# ----------------------------------------------------------------------\n",
    "TRAP_CELLS = {\n",
    "    rc_to_pos(0, 0),\n",
    "    rc_to_pos(3, 1),\n",
    "}\n",
    "CHEESE_CELLS = {\n",
    "    rc_to_pos(2, 1),\n",
    "    rc_to_pos(2, 3),\n",
    "}\n",
    "\n",
    "is_trap = np.zeros(NUM_CELLS, dtype=bool)\n",
    "is_cheese = np.zeros(NUM_CELLS, dtype=bool)\n",
    "for p in TRAP_CELLS:\n",
    "    is_trap[p] = True\n",
    "for p in CHEESE_CELLS:\n",
    "    is_cheese[p] = True\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Actions and motion dynamics\n",
    "# ----------------------------------------------------------------------\n",
    "NORTH, SOUTH, EAST, WEST = 0, 1, 2, 3\n",
    "ACTIONS = [NORTH, SOUTH, EAST, WEST]\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "ACTION_DELTAS = {\n",
    "    NORTH: (-1, 0),\n",
    "    SOUTH: ( 1, 0),\n",
    "    EAST:  ( 0, 1),\n",
    "    WEST:  ( 0,-1),\n",
    "}\n",
    "\n",
    "LEFT_OF = {\n",
    "    NORTH: WEST,\n",
    "    SOUTH: EAST,\n",
    "    EAST:  NORTH,\n",
    "    WEST:  SOUTH,\n",
    "}\n",
    "RIGHT_OF = {\n",
    "    NORTH: EAST,\n",
    "    SOUTH: WEST,\n",
    "    EAST:  SOUTH,\n",
    "    WEST:  NORTH,\n",
    "}\n",
    "\n",
    "def move_cell(pos, action):\n",
    "    r, c = pos_to_rc(pos)\n",
    "    dr, dc = ACTION_DELTAS[action]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if 0 <= nr < GRID_SIZE and 0 <= nc < GRID_SIZE:\n",
    "        return rc_to_pos(nr, nc)\n",
    "    else:\n",
    "        return pos  # bounce into wall\n",
    "\n",
    "def jerry_transition_probs(jerry_pos, action):\n",
    "    intended = action\n",
    "    left = LEFT_OF[action]\n",
    "    right = RIGHT_OF[action]\n",
    "    moves = [intended, left, right]\n",
    "    probs = [0.8, 0.1, 0.1]\n",
    "\n",
    "    outcomes = {}\n",
    "    for a, p in zip(moves, probs):\n",
    "        nxt = move_cell(jerry_pos, a)\n",
    "        outcomes[nxt] = outcomes.get(nxt, 0.0) + p\n",
    "    return outcomes\n",
    "\n",
    "def tom_transition_probs(tom_pos):\n",
    "    outcomes = {}\n",
    "    for a in ACTIONS:\n",
    "        nxt = move_cell(tom_pos, a)\n",
    "        outcomes[nxt] = outcomes.get(nxt, 0.0) + 1.0 / NUM_ACTIONS\n",
    "    return outcomes\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Joint state space: (Jerry_pos, Tom_pos)\n",
    "# ----------------------------------------------------------------------\n",
    "NUM_STATES = NUM_CELLS * NUM_CELLS\n",
    "\n",
    "def encode_state(jerry_pos, tom_pos):\n",
    "    return jerry_pos * NUM_CELLS + tom_pos\n",
    "\n",
    "def decode_state(s):\n",
    "    jerry_pos = s // NUM_CELLS\n",
    "    tom_pos = s % NUM_CELLS\n",
    "    return jerry_pos, tom_pos\n",
    "\n",
    "def is_bad_state(jerry_pos, tom_pos):\n",
    "    return is_trap[jerry_pos] or (jerry_pos == tom_pos)\n",
    "\n",
    "def is_goal_state(jerry_pos, tom_pos):\n",
    "    return is_cheese[jerry_pos]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5. Build transition matrices P (as np.array) and then convert to list\n",
    "# ----------------------------------------------------------------------\n",
    "def build_transition_matrices():\n",
    "    P = np.zeros((NUM_ACTIONS, NUM_STATES, NUM_STATES), dtype=np.float64)\n",
    "\n",
    "    # base dynamics\n",
    "    for jerry in range(NUM_CELLS):\n",
    "        for tom in range(NUM_CELLS):\n",
    "            s = encode_state(jerry, tom)\n",
    "\n",
    "            for a in ACTIONS:\n",
    "                j_trans = jerry_transition_probs(jerry, a)\n",
    "                t_trans = tom_transition_probs(tom)\n",
    "\n",
    "                for j_next, pj in j_trans.items():\n",
    "                    for t_next, pt in t_trans.items():\n",
    "                        s_next = encode_state(j_next, t_next)\n",
    "                        P[a, s, s_next] += pj * pt\n",
    "\n",
    "    # make goal and bad states absorbing\n",
    "    for s in range(NUM_STATES):\n",
    "        j, t = decode_state(s)\n",
    "        if is_goal_state(j, t) or is_bad_state(j, t):\n",
    "            P[:, s, :] = 0.0\n",
    "            for a in ACTIONS:\n",
    "                P[a, s, s] = 1.0\n",
    "\n",
    "    # convert 3D array -> list of (S,S) matrices for PyMDPtoolbox\n",
    "    P_list = [P[a] for a in range(NUM_ACTIONS)]\n",
    "    return P_list\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6. Reward vector R\n",
    "# ----------------------------------------------------------------------\n",
    "def build_reward_vector():\n",
    "    R = np.zeros(NUM_STATES, dtype=np.float64)\n",
    "    for s in range(NUM_STATES):\n",
    "        j, t = decode_state(s)\n",
    "        if is_goal_state(j, t):\n",
    "            R[s] = 1.0\n",
    "        else:\n",
    "            R[s] = 0.0\n",
    "    return R\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 7. Solve MDP with Value Iteration\n",
    "# ----------------------------------------------------------------------\n",
    "def solve_mdp(P_list, R, discount=0.99, epsilon=1e-5, max_iter=10_000):\n",
    "    vi = ValueIteration(P_list, R, discount=discount, epsilon=epsilon, max_iter=max_iter)\n",
    "    vi.run()\n",
    "    return np.array(vi.policy), np.array(vi.V)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 8. Aggregate values for Jerry only (marginalizing Tom)\n",
    "# ----------------------------------------------------------------------\n",
    "def jerry_value_marginal_over_tom(V, tom_initial_dist=None):\n",
    "    if tom_initial_dist is None:\n",
    "        tom_initial_dist = np.ones(NUM_CELLS) / NUM_CELLS\n",
    "    tom_initial_dist = np.asarray(tom_initial_dist)\n",
    "    assert tom_initial_dist.shape == (NUM_CELLS,)\n",
    "    assert np.allclose(tom_initial_dist.sum(), 1.0)\n",
    "\n",
    "    jerry_values = np.zeros(NUM_CELLS)\n",
    "    for j in range(NUM_CELLS):\n",
    "        val = 0.0\n",
    "        for t in range(NUM_CELLS):\n",
    "            s = encode_state(j, t)\n",
    "            val += tom_initial_dist[t] * V[s]\n",
    "        jerry_values[j] = val\n",
    "    return jerry_values.reshape(GRID_SIZE, GRID_SIZE)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 9. Main experiment\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Building transition matrices...\")\n",
    "    P_list = build_transition_matrices()\n",
    "    print(\"Done. Building rewards...\")\n",
    "    R = build_reward_vector()\n",
    "\n",
    "    print(\"Solving MDP with Value Iteration...\")\n",
    "    policy, V = solve_mdp(P_list, R, discount=0.99)\n",
    "    print(\"Done.\")\n",
    "    print(\"Policy shape:\", policy.shape)\n",
    "    print(\"Value function shape:\", V.shape)\n",
    "\n",
    "    # Example initial state\n",
    "    jerry_start = rc_to_pos(4, 0)\n",
    "    tom_start = rc_to_pos(0, 2)\n",
    "    s0 = encode_state(jerry_start, tom_start)\n",
    "    print(f\"Value at initial state (Jerry={jerry_start}, Tom={tom_start}): {V[s0]:.4f}\")\n",
    "\n",
    "    # Aggregate over random Tom initial position\n",
    "    grid_vals = jerry_value_marginal_over_tom(V)\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    print(\"\\nJerry's aggregated values over starting positions (5x5 grid):\")\n",
    "    print(grid_vals)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (formal-methods)",
   "language": "python",
   "name": "formal-methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
