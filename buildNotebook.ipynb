{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_8cFlzl3Ia2",
        "outputId": "d4e6ac32-e40f-4d50-933f-004d7c9521cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip -q install -U pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U numpy matplotlib"
      ],
      "metadata": {
        "id": "vcAUOk563Nf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tom_jerry_mdp.py\n",
        "# Temporal Logic–Constrained Control in MDPs\n",
        "# Spec: Avoid all traps and Tom while maximizing probability of reaching cheese.\n",
        "# This script compares Jerry's optimal policy/value under (1) random Tom, (2) heuristic-chasing Tom.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Grid setup and indexing\n",
        "# -----------------------------\n",
        "GRID_SIZE = 5\n",
        "NUM_CELLS = GRID_SIZE * GRID_SIZE\n",
        "\n",
        "def rc_to_pos(r, c):\n",
        "    return r * GRID_SIZE + c\n",
        "\n",
        "def pos_to_rc(pos):\n",
        "    return divmod(pos, GRID_SIZE)\n",
        "\n",
        "# Environment layout: traps and cheese (code coordinates: row 0 = top)\n",
        "TRAP_CELLS = {\n",
        "    rc_to_pos(0, 0),  # top-left\n",
        "    rc_to_pos(3, 2),  # lower trap\n",
        "}\n",
        "CHEESE_CELLS = {\n",
        "    rc_to_pos(2, 1),  # left cheese\n",
        "    rc_to_pos(2, 4),  # right cheese\n",
        "}\n",
        "\n",
        "is_trap = np.zeros(NUM_CELLS, dtype=bool)\n",
        "is_cheese = np.zeros(NUM_CELLS, dtype=bool)\n",
        "for p in TRAP_CELLS:\n",
        "    is_trap[p] = True\n",
        "for p in CHEESE_CELLS:\n",
        "    is_cheese[p] = True\n",
        "\n",
        "# -----------------------------\n",
        "# Actions and single-agent dynamics\n",
        "# -----------------------------\n",
        "NORTH, SOUTH, EAST, WEST = 0, 1, 2, 3\n",
        "ACTIONS = [NORTH, SOUTH, EAST, WEST]\n",
        "NUM_ACTIONS = len(ACTIONS)\n",
        "\n",
        "ACTION_DELTAS = {\n",
        "    NORTH: (-1, 0),\n",
        "    SOUTH: ( 1, 0),\n",
        "    EAST:  ( 0, 1),\n",
        "    WEST:  ( 0,-1),\n",
        "}\n",
        "LEFT_OF = {NORTH: WEST, SOUTH: EAST, EAST: NORTH, WEST: SOUTH}\n",
        "RIGHT_OF = {NORTH: EAST, SOUTH: WEST, EAST: SOUTH, WEST: NORTH}\n",
        "\n",
        "def move_cell(pos, action):\n",
        "    \"\"\"Deterministic move; if off-grid, stay.\"\"\"\n",
        "    r, c = pos_to_rc(pos)\n",
        "    dr, dc = ACTION_DELTAS[action]\n",
        "    nr, nc = r + dr, c + dc\n",
        "    if 0 <= nr < GRID_SIZE and 0 <= nc < GRID_SIZE:\n",
        "        return rc_to_pos(nr, nc)\n",
        "    else:\n",
        "        return pos\n",
        "\n",
        "def jerry_transition_probs(jerry_pos, action):\n",
        "    \"\"\"\n",
        "    Jerry: 0.8 intended direction, 0.1 left, 0.1 right.\n",
        "    Returns dict[next_jerry_pos] = prob.\n",
        "    \"\"\"\n",
        "    intended = action\n",
        "    left = LEFT_OF[action]\n",
        "    right = RIGHT_OF[action]\n",
        "    moves = [intended, left, right]\n",
        "    probs = [0.8, 0.1, 0.1]\n",
        "    out = {}\n",
        "    for a, p in zip(moves, probs):\n",
        "        nxt = move_cell(jerry_pos, a)\n",
        "        out[nxt] = out.get(nxt, 0.0) + p\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Tom policies\n",
        "# -----------------------------\n",
        "def tom_transition_probs_random(tom_pos):\n",
        "    \"\"\"Tom: uniform random N,S,E,W.\"\"\"\n",
        "    out = {}\n",
        "    for a in ACTIONS:\n",
        "        nxt = move_cell(tom_pos, a)\n",
        "        out[nxt] = out.get(nxt, 0.0) + 1.0 / NUM_ACTIONS\n",
        "    return out\n",
        "\n",
        "def tom_heuristic_action(tom_pos, jerry_pos):\n",
        "    \"\"\"\n",
        "    Manhattan heuristic toward Jerry.\n",
        "    Tie-breaking: if multiple actions give same min distance, break uniformly.\n",
        "    \"\"\"\n",
        "    jr, jc = pos_to_rc(jerry_pos)\n",
        "    best_actions = []\n",
        "    best_dist = float('inf')\n",
        "    for a in ACTIONS:\n",
        "        nxt = move_cell(tom_pos, a)\n",
        "        nr, nc = pos_to_rc(nxt)\n",
        "        dist = abs(nr - jr) + abs(nc - jc)\n",
        "        if dist < best_dist:\n",
        "            best_dist = dist\n",
        "            best_actions = [a]\n",
        "        elif dist == best_dist:\n",
        "            best_actions.append(a)\n",
        "    # uniform tie-break\n",
        "    return np.random.choice(best_actions) if len(best_actions) > 1 else best_actions[0]\n",
        "\n",
        "def tom_transition_probs_heuristic(tom_pos, jerry_pos, chase_p=0.7):\n",
        "    \"\"\"\n",
        "    Tom: chase Jerry with probability chase_p, otherwise uniform random (1 - chase_p).\n",
        "    Returns dict[next_tom_pos] = prob.\n",
        "    \"\"\"\n",
        "    chase_p = float(chase_p)\n",
        "    assert 0.0 <= chase_p <= 1.0\n",
        "    out = {}\n",
        "    # chase branch\n",
        "    best_action = tom_heuristic_action(tom_pos, jerry_pos)\n",
        "    nxt = move_cell(tom_pos, best_action)\n",
        "    out[nxt] = out.get(nxt, 0.0) + chase_p\n",
        "    # random branch\n",
        "    rand_p = (1.0 - chase_p) / NUM_ACTIONS\n",
        "    for a in ACTIONS:\n",
        "        nxt = move_cell(tom_pos, a)\n",
        "        out[nxt] = out.get(nxt, 0.0) + rand_p\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Joint state space\n",
        "# -----------------------------\n",
        "NUM_STATES = NUM_CELLS * NUM_CELLS\n",
        "\n",
        "def encode_state(jerry_pos, tom_pos):\n",
        "    return jerry_pos * NUM_CELLS + tom_pos\n",
        "\n",
        "def decode_state(s):\n",
        "    j = s // NUM_CELLS\n",
        "    t = s % NUM_CELLS\n",
        "    return j, t\n",
        "\n",
        "def is_bad_state(j, t):\n",
        "    return is_trap[j] or (j == t)\n",
        "\n",
        "def is_goal_state(j, t):\n",
        "    return is_cheese[j]\n",
        "\n",
        "# -----------------------------\n",
        "# Joint transition with caching\n",
        "# -----------------------------\n",
        "_TRANSITION_CACHE = {}\n",
        "\n",
        "def joint_transition(s, a, tom_policy='random', chase_p=0.7):\n",
        "    \"\"\"\n",
        "    Return dict[next_state] = prob under given action and Tom policy.\n",
        "    tom_policy: 'random' or 'heuristic'; chase_p only used for 'heuristic'.\n",
        "    \"\"\"\n",
        "    key = (s, a, tom_policy, round(float(chase_p), 6))\n",
        "    if key in _TRANSITION_CACHE:\n",
        "        return _TRANSITION_CACHE[key]\n",
        "\n",
        "    j, t = decode_state(s)\n",
        "    if is_goal_state(j, t) or is_bad_state(j, t):\n",
        "        out = {s: 1.0}\n",
        "        _TRANSITION_CACHE[key] = out\n",
        "        return out\n",
        "\n",
        "    out = {}\n",
        "    j_probs = jerry_transition_probs(j, a)\n",
        "    if tom_policy == 'random':\n",
        "        t_probs = tom_transition_probs_random(t)\n",
        "    elif tom_policy == 'heuristic':\n",
        "        t_probs = tom_transition_probs_heuristic(t, j, chase_p=chase_p)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown tom_policy: {tom_policy}\")\n",
        "\n",
        "    for j_next, pj in j_probs.items():\n",
        "        for t_next, pt in t_probs.items():\n",
        "            s_next = encode_state(j_next, t_next)\n",
        "            out[s_next] = out.get(s_next, 0.0) + pj * pt\n",
        "\n",
        "    _TRANSITION_CACHE[key] = out\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Value iteration (reachability)\n",
        "# -----------------------------\n",
        "def value_iteration_reachability(tom_policy='random', chase_p=0.7, tol=1e-7, max_iter=10_000, verbose=True):\n",
        "    \"\"\"\n",
        "    Solve V(s) = max_a E[V(s')] under boundary conditions:\n",
        "        V(goal) = 1, V(bad) = 0 (absorbing).\n",
        "    tom_policy: 'random' or 'heuristic'; chase_p for heuristic aggressiveness.\n",
        "    Returns (V, policy).\n",
        "    \"\"\"\n",
        "    V = np.zeros(NUM_STATES, dtype=float)\n",
        "\n",
        "    # Terminal masks\n",
        "    goal_mask = np.zeros(NUM_STATES, dtype=bool)\n",
        "    bad_mask = np.zeros(NUM_STATES, dtype=bool)\n",
        "    for s in range(NUM_STATES):\n",
        "        j, t = decode_state(s)\n",
        "        if is_goal_state(j, t):\n",
        "            goal_mask[s] = True\n",
        "        if is_bad_state(j, t):\n",
        "            bad_mask[s] = True\n",
        "\n",
        "    # boundary conditions\n",
        "    V[goal_mask] = 1.0\n",
        "    V[bad_mask] = 0.0\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        delta = 0.0\n",
        "        V_old = V.copy()\n",
        "\n",
        "        for s in range(NUM_STATES):\n",
        "            if goal_mask[s] or bad_mask[s]:\n",
        "                continue\n",
        "            best_q = -1.0\n",
        "            for a in ACTIONS:\n",
        "                q = 0.0\n",
        "                trans = joint_transition(s, a, tom_policy=tom_policy, chase_p=chase_p)\n",
        "                for s_next, p in trans.items():\n",
        "                    q += p * V_old[s_next]\n",
        "                if q > best_q:\n",
        "                    best_q = q\n",
        "            V[s] = best_q\n",
        "            delta = max(delta, abs(V[s] - V_old[s]))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Iter {it}, delta = {delta:.3e}\")\n",
        "        if delta < tol:\n",
        "            if verbose:\n",
        "                print(f\"Converged in {it} iterations.\")\n",
        "            break\n",
        "\n",
        "    # greedy policy extraction\n",
        "    policy = np.full(NUM_STATES, -1, dtype=int)\n",
        "    for s in range(NUM_STATES):\n",
        "        if goal_mask[s] or bad_mask[s]:\n",
        "            continue\n",
        "        best_a = NORTH\n",
        "        best_q = -1.0\n",
        "        for a in ACTIONS:\n",
        "            q = 0.0\n",
        "            trans = joint_transition(s, a, tom_policy=tom_policy, chase_p=chase_p)\n",
        "            for s_next, p in trans.items():\n",
        "                q += p * V[s_next]\n",
        "            if q > best_q:\n",
        "                best_q = q\n",
        "                best_a = a\n",
        "        policy[s] = best_a\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "# -----------------------------\n",
        "# Aggregation and ASCII policy\n",
        "# -----------------------------\n",
        "def jerry_value_marginal_over_tom(V, tom_initial_dist=None):\n",
        "    \"\"\"Aggregate V over a prior distribution on Tom's initial cell.\"\"\"\n",
        "    if tom_initial_dist is None:\n",
        "        tom_initial_dist = np.ones(NUM_CELLS) / NUM_CELLS\n",
        "    tom_initial_dist = np.asarray(tom_initial_dist)\n",
        "    assert tom_initial_dist.shape == (NUM_CELLS,)\n",
        "    assert np.allclose(tom_initial_dist.sum(), 1.0)\n",
        "\n",
        "    jerry_vals = np.zeros(NUM_CELLS)\n",
        "    for j in range(NUM_CELLS):\n",
        "        val = 0.0\n",
        "        for t in range(NUM_CELLS):\n",
        "            s = encode_state(j, t)\n",
        "            val += tom_initial_dist[t] * V[s]\n",
        "        jerry_vals[j] = val\n",
        "    return jerry_vals.reshape(GRID_SIZE, GRID_SIZE)\n",
        "\n",
        "def jerry_policy_from_joint_ascii(V, tom_policy='random', chase_p=0.7, tom_initial_dist=None):\n",
        "    \"\"\"\n",
        "    Aggregated arrows for Jerry by averaging Q over Tom's initial distribution.\n",
        "    Returns (arrows_code, arrows_figure).\n",
        "    \"\"\"\n",
        "    if tom_initial_dist is None:\n",
        "        tom_initial_dist = np.ones(NUM_CELLS) / NUM_CELLS\n",
        "    tom_initial_dist = np.asarray(tom_initial_dist)\n",
        "    assert tom_initial_dist.shape == (NUM_CELLS,)\n",
        "    assert np.allclose(tom_initial_dist.sum(), 1.0)\n",
        "\n",
        "    arrow_for_action = {NORTH: \"↑\", SOUTH: \"↓\", EAST: \"→\", WEST: \"←\"}\n",
        "    arrows_code = np.empty((GRID_SIZE, GRID_SIZE), dtype=object)\n",
        "\n",
        "    for j in range(NUM_CELLS):\n",
        "        best_a = NORTH\n",
        "        best_q = -1.0\n",
        "        for a in ACTIONS:\n",
        "            q = 0.0\n",
        "            for t in range(NUM_CELLS):\n",
        "                s = encode_state(j, t)\n",
        "                trans = joint_transition(s, a, tom_policy=tom_policy, chase_p=chase_p)\n",
        "                for s_next, p in trans.items():\n",
        "                    q += tom_initial_dist[t] * p * V[s_next]\n",
        "            if q > best_q:\n",
        "                best_q = q\n",
        "                best_a = a\n",
        "        r, c = pos_to_rc(j)\n",
        "        arrows_code[r, c] = arrow_for_action[best_a]\n",
        "\n",
        "    arrows_figure = np.flipud(arrows_code)  # match figure convention (row 0 at bottom)\n",
        "    return arrows_code, arrows_figure\n",
        "\n",
        "# -----------------------------\n",
        "# Visualization helpers\n",
        "# -----------------------------\n",
        "def add_markers(ax):\n",
        "    \"\"\"Overlay traps (X) and cheese (C) on a heatmap axes.\"\"\"\n",
        "    for p in TRAP_CELLS:\n",
        "        r, c = pos_to_rc(p)\n",
        "        ax.text(c, r, 'X', color='black', fontsize=14, ha='center', va='center', fontweight='bold')\n",
        "    for p in CHEESE_CELLS:\n",
        "        r, c = pos_to_rc(p)\n",
        "        ax.text(c, r, 'C', color='black', fontsize=14, ha='center', va='center', fontweight='bold')\n",
        "\n",
        "def plot_comparison(grid_vals_random, grid_vals_heuristic, save_path=\"comparison.png\"):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    im1 = axes[0].imshow(grid_vals_random, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "    axes[0].set_title(\"Random Tom\")\n",
        "    add_markers(axes[0])\n",
        "    fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "\n",
        "    im2 = axes[1].imshow(grid_vals_heuristic, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "    axes[1].set_title(\"Heuristic Tom\")\n",
        "    add_markers(axes[1])\n",
        "    fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "    diff = grid_vals_random - grid_vals_heuristic\n",
        "    vlim = max(abs(diff.min()), abs(diff.max()))\n",
        "    im3 = axes[2].imshow(diff, cmap='coolwarm', vmin=-vlim, vmax=vlim)\n",
        "    axes[2].set_title(\"Advantage of Random Tom (Δ)\")\n",
        "    add_markers(axes[2])\n",
        "    fig.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_xlabel(\"Col\")\n",
        "        ax.set_ylabel(\"Row\")\n",
        "        ax.set_xticks(range(GRID_SIZE))\n",
        "        ax.set_yticks(range(GRID_SIZE))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Main experiment\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "    print(\"Solving with random Tom...\")\n",
        "    V_random, policy_random = value_iteration_reachability(tom_policy='random', tol=1e-7, verbose=True)\n",
        "    grid_vals_random = jerry_value_marginal_over_tom(V_random)\n",
        "    print(\"\\nJerry success probabilities (random Tom):\")\n",
        "    print(grid_vals_random)\n",
        "\n",
        "    print(\"\\nSolving with heuristic Tom (chase_p=0.7)...\")\n",
        "    V_heuristic, policy_heuristic = value_iteration_reachability(tom_policy='heuristic', chase_p=0.7, tol=1e-7, verbose=True)\n",
        "    grid_vals_heuristic = jerry_value_marginal_over_tom(V_heuristic)\n",
        "    print(\"\\nJerry success probabilities (heuristic Tom):\")\n",
        "    print(grid_vals_heuristic)\n",
        "\n",
        "    # difference\n",
        "    diff = grid_vals_random - grid_vals_heuristic\n",
        "    print(\"\\nSuccess probability decrease with heuristic Tom (random - heuristic):\")\n",
        "    print(diff)\n",
        "\n",
        "    # sample initial state value\n",
        "    jerry_start = rc_to_pos(4, 0)\n",
        "    tom_start = rc_to_pos(0, 2)\n",
        "    s0 = encode_state(jerry_start, tom_start)\n",
        "    print(f\"\\nSample V_random at (Jerry={jerry_start}, Tom={tom_start}): {V_random[s0]:.4f}\")\n",
        "    print(f\"Sample V_heuristic at (Jerry={jerry_start}, Tom={tom_start}): {V_heuristic[s0]:.4f}\")\n",
        "\n",
        "    # ASCII arrows (aggregated policy) under each Tom policy\n",
        "    arrows_code_rand, _ = jerry_policy_from_joint_ascii(V_random, tom_policy='random')\n",
        "    print(\"\\nJerry aggregated optimal directions (random Tom):\")\n",
        "    for r in range(GRID_SIZE):\n",
        "        print(\" \".join(arrows_code_rand[r]))\n",
        "\n",
        "    arrows_code_heur, _ = jerry_policy_from_joint_ascii(V_heuristic, tom_policy='heuristic', chase_p=0.7)\n",
        "    print(\"\\nJerry aggregated optimal directions (heuristic Tom):\")\n",
        "    for r in range(GRID_SIZE):\n",
        "        print(\" \".join(arrows_code_heur[r]))\n",
        "\n",
        "    # Visualization\n",
        "    print(\"\\nRendering comparison heatmaps...\")\n",
        "    plot_comparison(grid_vals_random, grid_vals_heuristic, save_path=\"comparison.png\")\n",
        "\n",
        "    # Optional: save CSVs\n",
        "    np.savetxt(\"jerry_success_random.csv\", grid_vals_random, delimiter=\",\", fmt=\"%.4f\")\n",
        "    np.savetxt(\"jerry_success_heuristic.csv\", grid_vals_heuristic, delimiter=\",\", fmt=\"%.4f\")\n",
        "    np.savetxt(\"jerry_success_diff.csv\", diff, delimiter=\",\", fmt=\"%.4f\")"
      ],
      "metadata": {
        "id": "2jJAVjpl3Pfg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}